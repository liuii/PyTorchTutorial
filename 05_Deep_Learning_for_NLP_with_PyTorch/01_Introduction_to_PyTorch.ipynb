{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "origin: http://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html  \n",
    "translator: Hongpu Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch简介\n",
    "## 1. Torch张量库的介绍\n",
    "所有的深度学习都在对张量进行计算，张量可以看成是矩阵的推广，通常张量的维度大于2维。后面我们将深入的了解到其含义，首先看一下可以对张量做什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f935b544ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 创建张量\n",
    "张量可以通过调用**torch.Tensor()**从Python列表来创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据给定的数据创建一个torch.Tensor对象，它是一个1D向量\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.Tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# 创建一个矩阵\n",
    "M_data = [[1., 2., 3.], [4., 5., 6.]]\n",
    "M = torch.Tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# 创建一个2x2x2的3D张量\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是3D张量？如果你有一个向量，向量的元素是一个标量。如果你有一个矩阵，矩阵的元素是一个向量。如果你有一个3D张量，它的每一个元素都是一个矩阵！\n",
    "\n",
    "**术语**：当我们说“张量”时，它指的是任何的**torch.Tensor**对象。矩阵和向量是**torch.Tensor**的特殊情况，它们的维度分别是2和1.当我们讨论3D张量时，我们会显式的使用术语“3D张量”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# V的元素是一个标量\n",
    "print(V[0])\n",
    "\n",
    "# M的元素是一个向量\n",
    "print(M[0])\n",
    "\n",
    "# T的元素是一个矩阵\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以创建其他数据类型的张量。默认情况下是**Float**类型。要创建一个整数类型的张量，可以使用**torch.LongTensor()**。查看文档可以看到更多的数据类型，但是**Float**和**Long**是最常用的。\n",
    "\n",
    "你可以用**torch.randn()**来创建包含随机数据的张量，函数的参数是张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.6614  0.2669  0.0617  0.6213 -0.4519\n",
      " -0.1661 -1.5228  0.3817 -1.0276 -0.5631\n",
      " -0.8923 -0.0583 -0.1955 -0.9656  0.4224\n",
      "  0.2673 -0.4212 -0.5107 -1.5727 -0.1232\n",
      "\n",
      "(1 ,.,.) = \n",
      "  3.5870 -1.8313  1.5987 -1.2770  0.3255\n",
      " -0.4791  1.3790  2.5286  0.4107 -0.9880\n",
      " -0.9081  0.5423  0.1103 -2.2590  0.6067\n",
      " -0.1383  0.8310 -0.2477 -0.8029  0.2366\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.2857  0.6898 -0.6331  0.8795 -0.6842\n",
      "  0.4533  0.2912 -0.8317 -0.5525  0.6355\n",
      " -0.3968 -0.6571 -1.6428  0.9803 -0.0421\n",
      " -0.8206  0.3133 -1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 3x4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 张量的运算\n",
    "你可以按照你期望的方式来操作张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "y = torch.Tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量操作的完整列表可以查阅[文档](http://pytorch.org/docs/torch.html)，你会看到大量的张量运算。张量的运算不仅仅支持数学运算。\n",
    "\n",
    "后面我们会常用的一个操作是数据的串联。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.5667 -1.4303  0.5009  0.5438 -0.4057\n",
      " 1.1341 -1.1115  0.3501 -0.7703 -0.1473\n",
      " 0.6272  1.0935  0.0939  1.2381 -1.3459\n",
      " 0.5119 -0.6933 -0.1668 -0.9999 -1.6476\n",
      " 0.8098  0.0554  1.1340 -0.5326  0.6592\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      "-1.5964 -0.3769 -3.1020 -0.0020 -1.0952  0.6016  0.6984 -0.8005\n",
      "-0.0995 -0.7213  1.2708  1.5381  1.4673  1.5951 -1.5279  1.0156\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 默认情况下，串联数据发生在第一个轴（按行串联）\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# 按列串联\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# 第二个参数指定了进行拼接的轴\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# 如果拼接的张量维度不匹配，将会出错。把下面语句的注释取消可以看到错误\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 改变张量的形状\n",
    "使用**.view()**方法可以改变张量的形状。由于很多神经网络的组件需要特定的输入形状，因此这个方法会被频繁的使用。经常在传递数据给组件之前需要改变数据的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -0.2020 -1.2865  0.8231 -0.6101\n",
      " -1.2960 -0.9434  0.6684  1.1628\n",
      " -0.3229  1.8782 -0.5666  0.4016\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.1153  0.3170  0.5629  0.8662\n",
      " -0.3528  0.3482  1.1371 -0.3339\n",
      " -1.4724  0.7296 -0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12)) # 改变为2行、12列的张量\n",
    "\n",
    "# 下面与上面的代码结果一样，如果其中一个维度为-1，该维度的值会自动进行计算\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 计算图与自动微分\n",
    "计算图是提高深度学习编程的效率的核心，因为它允许你无需自己编写反向梯度传播。计算图指定了你的数据是如何组合运算得到输出的。由于计算图完整包含了运算所涉及的参数，因此它为计算微分提供了充足的信息。这听上去含糊不清，因此让我们看一下在使用PyTorch的基础类**autograd.Variable**时发生了什么。\n",
    "\n",
    "首先从程序员的视角来看。我们之前创建的**torch.Tensor**到底存储了什么？显然包括数据和形状，以及其他一些东西。但是当我们把两个张量进行相加时，得到了输出张量。这个输出张量只知道它的数据和形状。它并不知道自己是两个其他张量的和（因为可能是从文件中读取到的，也可能是其他运算你的结果）。\n",
    "\n",
    "**Variable**可以对它是如何创建的保持跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<AddBackward1 object at 0x7f93353c2890>\n"
     ]
    }
   ],
   "source": [
    "# 变量包含了张量对象\n",
    "x = autograd.Variable(torch.Tensor([1., 2., 3.]), requires_grad=True)\n",
    "\n",
    "# 可以通过 .data 属性来访问变量中的数据\n",
    "print(x.data)\n",
    "\n",
    "# 你也可以把那些用于张量的运算用于变量\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6.]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# 但是z保存了更多的信息\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "因此变量知道自己是如何被创建的。z知道自己不是从文件中读取的，也不是乘法或指数运算的结果。沿着**z.grad_fn**将找到x和y。\n",
    "\n",
    "但是它是如何帮助我们计算梯度的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x7f93303d6c50>\n"
     ]
    }
   ],
   "source": [
    "# 让我们把z的所有项进行求和\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "s关于x的第一个元素的微分是什么？用数学表达式可写为：\n",
    "$$\\frac{\\partial s}{\\partial x_0}$$\n",
    "\n",
    "现在**s**知道它是**z**所有元素的和。而**z**指导它是**x**和**y**的和。也就是：\n",
    "\n",
    "$$s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}$$\n",
    "\n",
    "**s**中包含了足够的信息来确定我们要求的微分是**1**！\n",
    "\n",
    "当然这掩盖了实际上计算微分的难度。重要的是知道，变量保存了足够的信息用于计算梯度。实际上，Pytorch的开发者在编写**sum()**和**+**运算时，知道如何计算它们的梯度以及如何运行反向传播算法。更深入的讨论超出了本教程的范围。\n",
    "\n",
    "接下来让Pytorch来计算梯度（注意，如果运行多次的话，梯度将会递增。这是因为Pytorch会把梯度**累加**到**.grad**属性中，这对许多模型非常方便）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用任意变量的 .backward() 将会运行反向传播\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "理解下面代码发生了什么，对于成为一个成功深度学习开发者非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x7f93303d6e50>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "z = x + y # 这些是Tensor，无法进行反向传播\n",
    "\n",
    "var_x = autograd.Variable(x, requires_grad=True)\n",
    "var_y = autograd.Variable(y, requires_grad=True)\n",
    "# var_z包含了足够信息来计算梯度\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)\n",
    "\n",
    "var_z_data = var_z.data # 获得var_z中封装的张量对象\n",
    "# 将张量重新封装到一个变量中\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "\n",
    "# new_var_z是否拥有反向传播计算x和y梯度的信息呢？\n",
    "# 没有！\n",
    "print(new_var_z.grad_fn)\n",
    "# 因为我们把Tensor对象从var_z中取出，该Tensor并不包含它是如何被计算出来的信息。\n",
    "# 我们把这个Tensor传递给new_var_z，这是new_var_z获得的唯一信息。由于var_z_data\n",
    "# 并不知道自己是被如何计算出来的，new_var_z同样不会知道。\n",
    "# 本质上，我们打断了变量的运算历史。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下面的规则，对于用**autograd.Variable**来进行计算非常重要（注意：**autograd.Variable**在其他的主要深度学习工具集中都有等价的对象）：\n",
    "\n",
    "> 如果要将损失函数的误差传播到网络中的一个组件，你就**绝对不能**打断从该组件到损失变量之间的计算链。如果你这样做了，损失将不知道该组件的存在，组件的权重也就无法更新。\n",
    "\n",
    "上面的错误会悄悄地影响你的代码的执行结果，而且不会导致代码崩溃或警告，因此必须谨慎对待。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
