{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Origin : http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html  \n",
    "Translator : Hongpu Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd：自动微分\n",
    "对于PyTorch中所有神经网络来说，最核心的就是``autograd``包。让我们首先简单看一下它的使用，然后训练我们第一个神经网络。\n",
    "\n",
    "``autograd``包对所有张量操作提供了自动微分功能。它是一种运行时定义框架，意味着你的反向传播在运行时定义，因此每次迭代都可以不同。\n",
    "\n",
    "**补充：PyTorch是动态图，运行时建图；而静态图在运行前需要编译，运行时图的结构是不能改变的。**\n",
    "\n",
    "下面通过几个例子来展示其中概念。\n",
    "\n",
    "## 1. 变量（Variable）\n",
    "**autograd.Variable**是**autograd**的核心类。它包含张量，并支持集合所有定义在张量之上的操作。一旦但成了计算，就可以调用**.backward()**自动计算梯度。\n",
    "\n",
    "可以通过**.data**属性来调用Variable中的张量，而关于变量的梯度将会累加到**.grad**属性中。\n",
    "\n",
    "![](./imgs/AutoGrade1.png)\n",
    "\n",
    "**Function**是实现自动梯度功能另外一个重要的类。\n",
    "\n",
    "**Variable**和**Function**互相连接来构建一个无环图，该无环图将所有计算过程进行编码。每个变量有一个**.grad_fn**属性，该属性用来引用创建**Variable**的**Function**（除了用户创建的变量，用户创建变量的**grad_fn**是**None**）。\n",
    "\n",
    "如果需要计算微分，可以调用**Variable**的**.backward()**。如果**Variable**是标量（只包含一个数据元素），不需要为**.backward()**指定任何参数。反之，需要制定**gradient**参数的值，需要将一个与梯度维度相同的张量传递给它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对变量做一个操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**y**是由操作创建的结果，因此它有一个**grad_fn**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fa4ccd50210>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对**y**做更多的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 梯度（Gradients）\n",
    "现在来进行反向传播，**out.backward()**等价于**out.backward(torch.Tensor([1.0]))**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印梯度**d(out)/dx**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的到一个矩阵所有值为**4.5**。\n",
    "设**out**变量为$o$，有：\n",
    "\n",
    "前馈过程：\n",
    "$$o = \\frac{1}{4}\\sum_i z_i$$\n",
    "$$z_i = 3(x_i+2)^2$$\n",
    "$$z_i\\bigr\\rvert_{x_i=1} = 27$$\n",
    "\n",
    "梯度：\n",
    "$$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$$\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$$\n",
    "\n",
    "还可以利用**autograd**做更疯狂的事情！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1665.5319\n",
      "  291.7892\n",
      "  -88.4084\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**补充**  \n",
    "\n",
    "若调用为*y.backward(gradient=grads)*，表示：\n",
    "\n",
    "$$\\frac {\\partial {obj}} {\\partial z} \\frac {\\partial z} {\\partial \\omega} = grads \\times \\frac {\\partial z} {\\partial \\omega}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  51.2000\n",
      " 512.0000\n",
      "   0.0512\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 因为y是一个向量函数，因此在backward是要传递gradient参数，参数中对应的值表示其梯度的比例\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "\n",
    "# 注意每次backward产生的梯度都会累加到x.grad上，因此反复执行这个代码块，会看到梯度在不断增加\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**补充阅读**\n",
    "\n",
    "**Variable**和**Function**的文档可以参阅：[http://pytorch.org/docs/autograd](http://pytorch.org/docs/autograd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
